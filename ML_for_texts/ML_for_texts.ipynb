{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим датасет, познакомимся с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/admin_mbp_15_2012/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/admin_mbp_15_2012/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объявим константу:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузим датасет**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:**\n",
    "\n",
    "**Данные в таблице представлены двумя типами данных:**\n",
    "* object\n",
    "* int64\n",
    "\n",
    "**Столбец `text` содержит текст комментария, а столбец `toxic` — целевой признак.**\n",
    "\n",
    "**В данных нету пропусков.**\n",
    "\n",
    "**В названии колонкок нету нарушения стиля записи.**\n",
    "\n",
    "**Данные в колонке 'Unnamed: 0' повторяют номера индексов, эта колонка с неинформативным признаком, удалим ее в дальнейшем:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Неинформативные признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Удалим колонку 'Unnamed: 0' с неиформативным признаком:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверим результат:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дубликаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверим есть ли в данных явные дубликаты:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Явных дубликатов не обнаружили.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деление на выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выделим признак и целевой признак:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('toxic', axis=1)\n",
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Поделим данные на выборки:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, test_size=0.25, stratify=target, random_state=RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.reset_index(drop= True , inplace= True )\n",
    "features_test.reset_index(drop= True , inplace= True )\n",
    "target_train.reset_index(drop= True , inplace= True )\n",
    "target_test.reset_index(drop= True , inplace= True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим функцию для лемматизации текстов:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_text(text):\n",
    "    \n",
    "    final = []\n",
    "    for t in notebook.tqdm(text):\n",
    "        t = \" \".join(re.sub(r'[^a-zA-Z]', ' ', t).split())\n",
    "        token_list = word_tokenize(t)\n",
    "        \n",
    "        stop_text = []\n",
    "        for word in token_list:\n",
    "            if word.lower() not in stopwords.words('english'):\n",
    "                stop_text.append(word.lower())\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemm_list = []\n",
    "        for w in stop_text:\n",
    "            lem_t = lemmatizer.lemmatize(w)\n",
    "            lemm_list.append(lem_t)\n",
    "        lemm_list = \" \".join(lemm_list)\n",
    "        final.append(lemm_list)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Применим функцию, к новому столбцу, который создадим в тренировочной выборке:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bca24396bf4900aa8772639c683be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_train['lemm_text'] = lem_text(features_train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация длилась 27 минут, посмотрим результат:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SA article \\n\\nI'm not just blanking whatever ...</td>\n",
       "      <td>sa article blanking whatever want removing eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" \\n\\n Timeline February 8, 2005: Essjay accou...</td>\n",
       "      <td>timeline february essjay account registered es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\nVandalism was added with the promorion of s...</td>\n",
       "      <td>vandalism added promorion show people initial ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  SA article \\n\\nI'm not just blanking whatever ...   \n",
       "1  \" \\n\\n Timeline February 8, 2005: Essjay accou...   \n",
       "2  \"\\nVandalism was added with the promorion of s...   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  sa article blanking whatever want removing eve...  \n",
       "1  timeline february essjay account registered es...  \n",
       "2  vandalism added promorion show people initial ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Применим функцию, к новому столбцу, который создадим в тестовой выборке:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8ad27311f842dcac73d929a30c555d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 21s, sys: 1min 53s, total: 8min 15s\n",
      "Wall time: 8min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_test['lemm_text'] = lem_text(features_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация длилась 9 минут, посмотрим результат:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give me an email address where I may email you...</td>\n",
       "      <td>give email address may email concern please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\n\\n Genetics \\n\\nThis recent edit added to t...</td>\n",
       "      <td>genetics recent edit added genetics section tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012 (UTC)\\n You expected more from a liberal ...</td>\n",
       "      <td>utc expected liberal rag like wakopedia racist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Give me an email address where I may email you...   \n",
       "1  \"\\n\\n Genetics \\n\\nThis recent edit added to t...   \n",
       "2  2012 (UTC)\\n You expected more from a liberal ...   \n",
       "\n",
       "                                           lemm_text  \n",
       "0        give email address may email concern please  \n",
       "1  genetics recent edit added genetics section tw...  \n",
       "2  utc expected liberal rag like wakopedia racist...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корпусы текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим корпусы текстов для обучающей и тестовой выборок:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = list(features_train['lemm_text'].values)\n",
    "corpus_test = list(features_test['lemm_text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вычислим TF-IDF для корпусов тренировочной и тестовой выборок:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/admin_mbp_15_2012/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf_train = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = count_tf_idf_train.fit_transform(corpus_train)\n",
    "tf_idf_test = count_tf_idf_train.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод**\n",
    "\n",
    "**В ходе подготовки мы выполнили такие шаги:**\n",
    "- познакомились с датасетом\n",
    "- удалили неинформативный признак\n",
    "- выяснили, что нету пропусков\n",
    "- выяснили, что нет дубликатов\n",
    "- сделали лемматизацию текстов\n",
    "- подготовили корпусы текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**С помощью ранее созданного корпуса обучим разные модели:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим модель логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin_mbp_15_2012/opt/anaconda3/envs/ds_practicum_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 мера = 1\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('clf', LogisticRegression(random_state=RS))])\n",
    " \n",
    "scores = cross_val_score(pipe_lr, corpus_train, target_train,\n",
    "                                     scoring = 'f1', cv=3, n_jobs=-1).mean()\n",
    " \n",
    "model_lr = pipe_lr.fit(corpus_train, target_train)\n",
    "print(\"F1 мера =\", round(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим модель случайного леса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_f1(x_train, y_train):\n",
    "    model = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('clf', RandomForestClassifier(random_state=RS))])\n",
    "    \n",
    "    parametrs = {'clf__n_estimators': range (1, 15, 3),\n",
    "                 'clf__max_depth': range (1, 15, 3), \n",
    "                 'clf__min_samples_split': range (2, 9, 2)}\n",
    "    \n",
    "    model_rfr = GridSearchCV(model, parametrs, cv=3, scoring = 'f1', n_jobs=-1)\n",
    "    model_rfr.fit(x_train, y_train)\n",
    "    return model_rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели = {'clf__max_depth': 13, 'clf__min_samples_split': 6, 'clf__n_estimators': 1} F1 = 0\n",
      "CPU times: user 24 s, sys: 14.3 s, total: 38.4 s\n",
      "Wall time: 6min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_rfr = rfr_f1(corpus_train, target_train)\n",
    "print('Лучшие параметры модели =', model_rfr.best_params_, 'F1 =', round(model_rfr.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим модель решающего дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_f1(x_train, y_train):\n",
    "    model = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                 ('clf', DecisionTreeClassifier(random_state=RS))])\n",
    "    \n",
    "    parametrs = {'clf__max_depth': range (1, 15, 3), \n",
    "                 'clf__min_samples_split': range (2, 9, 2)}\n",
    "    \n",
    "    model_tree = GridSearchCV(model, parametrs, cv=3, scoring = 'f1', n_jobs=-1)\n",
    "    model_tree.fit(x_train, y_train)\n",
    "    return model_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели = {'clf__max_depth': 13, 'clf__min_samples_split': 6} F1 = 1\n",
      "CPU times: user 14.9 s, sys: 2.87 s, total: 17.8 s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_tree = tree_f1(corpus_train, target_train)\n",
    "print('Лучшие параметры модели =', model_tree.best_params_, 'F1 =', round(model_tree.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим модель CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost(x_train, y_train):\n",
    "    cat = CatBoostClassifier(random_state=RS,\n",
    "                            loss_function='Logloss',\n",
    "                            early_stopping_rounds=5,\n",
    "                            iterations=300,\n",
    "                            verbose=100)\n",
    "    \n",
    "    pipe_cat = Pipeline([('tfidf', TfidfVectorizer()), \n",
    "                         ('clf', cat)])\n",
    "        \n",
    "    scores_cat = cross_val_score(pipe_cat, x_train, y_train,\n",
    "                                     scoring = 'f1', cv=3, n_jobs=-1).mean()\n",
    "    \n",
    "    pipe_cat.fit(x_train, y_train)\n",
    "    print(\"F1 мера =\", scores_cat)\n",
    "    return pipe_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.20147\n",
      "0:\tlearn: 0.5101545\ttotal: 3.9s\tremaining: 19m 25s\n",
      "100:\tlearn: 0.1408232\ttotal: 2m 19s\tremaining: 4m 35s\n",
      "200:\tlearn: 0.1203324\ttotal: 4m 33s\tremaining: 2m 14s\n",
      "299:\tlearn: 0.1085701\ttotal: 6m 44s\tremaining: 0us\n",
      "Learning rate set to 0.20147\n",
      "0:\tlearn: 0.5093267\ttotal: 4.02s\tremaining: 20m 2s\n",
      "100:\tlearn: 0.1403395\ttotal: 2m 20s\tremaining: 4m 35s\n",
      "200:\tlearn: 0.1203905\ttotal: 4m 34s\tremaining: 2m 15s\n",
      "299:\tlearn: 0.1083183\ttotal: 6m 45s\tremaining: 0us\n",
      "Learning rate set to 0.20147\n",
      "0:\tlearn: 0.5158732\ttotal: 3.96s\tremaining: 19m 42s\n",
      "100:\tlearn: 0.1412694\ttotal: 2m 20s\tremaining: 4m 36s\n",
      "200:\tlearn: 0.1202942\ttotal: 4m 34s\tremaining: 2m 15s\n",
      "299:\tlearn: 0.1081968\ttotal: 6m 45s\tremaining: 0us\n",
      "Learning rate set to 0.239553\n",
      "0:\tlearn: 0.4720626\ttotal: 861ms\tremaining: 4m 17s\n",
      "100:\tlearn: 0.1373357\ttotal: 1m 12s\tremaining: 2m 23s\n",
      "200:\tlearn: 0.1179734\ttotal: 2m 24s\tremaining: 1m 11s\n",
      "299:\tlearn: 0.1075454\ttotal: 3m 35s\tremaining: 0us\n",
      "F1 мера = 0.7445378965577891\n",
      "CPU times: user 22min 6s, sys: 22.3 s, total: 22min 29s\n",
      "Wall time: 10min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cat_model =  catboost(corpus_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**На кросс-валидации получили F1-меру = 0.74**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузка предобученной модели DistilBERT и загрузка токенизатора:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel,\n",
    "                                                    ppb.DistilBertTokenizer,\n",
    "                                                    'distilbert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выберем небольшую часть датасета случайным образом:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert = df.copy()\n",
    "df_bert = df_bert.sample(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выполним токенизацию:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 s, sys: 21.2 ms, total: 2.3 s\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized = df_bert['text'].apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=150))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Приведем векторы к одному размеру путем добавления нулей до длинны максимально длинного вектора:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 150)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим маску, чтобы модель в дальнейшем не учитывала нули, которыми мы уравняли векторы в предыдущем пункте:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 150)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание входного вектора и передача его в модель DistilBERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 52s, sys: 36.7 s, total: 6min 28s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получим массив из эмбеддингов в качестве признаков:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Отделим целевой признак и получим выборки:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_target = df_bert['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_features, bert_test_features, bert_train_target, bert_test_target = train_test_split(\n",
    "    bert_features,\n",
    "    bert_target,\n",
    "    test_size=0.5,\n",
    "    random_state=RS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучим логистическую регресию на полученных выборках и найдем F1-меру:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bert = LogisticRegression()\n",
    "lr_bert.fit(bert_train_features, bert_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 мера = 1\n"
     ]
    }
   ],
   "source": [
    "pred_bert = lr_bert.predict(bert_test_features)\n",
    "f1_bert = f1_score(pred_bert, bert_test_target)\n",
    "print('F1 мера =', round(f1_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:**\n",
    "\n",
    "**Провели обучение и сравнили F1-меру таких моделей:**\n",
    "- LogisticRegression\n",
    "- DecisionTreeClassifier\n",
    "- RandomForestClassifier\n",
    "- CatBoostClassifier\n",
    "\n",
    "**По результатам сравнения F1-меры, лучшей моделью будем считать - CatBoostClassifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOXIC-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузка предобученной модели TOXIC-BERT и загрузка токенизатора:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_toxic = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "model_toxic = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at unitary/toxic-bert were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class_t, tokenizer_class_t, pretrained_weights_t = (ppb.BertModel,\n",
    "                                                    ppb.BertTokenizer,\n",
    "                                                    'unitary/toxic-bert')\n",
    "\n",
    "tokenizer_toxic = tokenizer_class_t.from_pretrained(pretrained_weights_t)\n",
    "model_toxic = model_class_t.from_pretrained(pretrained_weights_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выберем небольшую часть датасета случайным образом:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert_toxic = df.copy()\n",
    "df_bert_toxic = df_bert_toxic.sample(300).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выполним токенизацию:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 682 ms, sys: 9.4 ms, total: 691 ms\n",
      "Wall time: 691 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_toxic = df_bert_toxic['text'].apply(\n",
    "    (lambda x: tokenizer_toxic.encode(x, add_special_tokens=True, max_length=50))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Приведем векторы к одному размеру путем добавления нулей до длинны максимально длинного вектора:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized_toxic.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_toxic = np.array([i + [0]*(max_len-len(i)) for i in tokenized_toxic.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверим размер:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 50)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded_toxic).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим маску, чтобы модель в дальнейшем не учитывала нули, которыми мы уравняли векторы в предыдущем пункте:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 50)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_toxic = np.where(padded_toxic != 0, 1, 0)\n",
    "attention_mask_toxic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание входного вектора и передача его в модель TOXIC-BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 3.23 s, total: 1min 10s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_ids_toxic = torch.tensor(padded_toxic)  \n",
    "attention_mask_toxic = torch.tensor(attention_mask_toxic)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states_toxic = model_toxic(input_ids_toxic, attention_mask=attention_mask_toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получим массив из эмбеддингов в качестве признаков:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_features_toxic = last_hidden_states_toxic[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Отделим целевой признак и получим выборки:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_target_toxic = df_bert_toxic['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_features_t, bert_test_features_t, bert_train_target_t, bert_test_target_t = train_test_split(\n",
    "    bert_features_toxic,\n",
    "    bert_target_toxic,\n",
    "    test_size=0.5,\n",
    "    random_state=RS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучим логистическую регресию на полученных выборках и найдем F1-меру:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bert_tox = LogisticRegression()\n",
    "lr_bert_tox.fit(bert_train_features_t, bert_train_target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 мера = 1\n"
     ]
    }
   ],
   "source": [
    "pred_bert_toxic = lr_bert_tox.predict(bert_test_features_t)\n",
    "f1_bert_toxic = f1_score(pred_bert_toxic, bert_test_target_t)\n",
    "print('F1 мера =', round(f1_bert_toxic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проведем тестирование, найденной лучшей модели на тестовой выбоке:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 модели = 0.7623563218390805\n"
     ]
    }
   ],
   "source": [
    "pred_cat_test = cat_model.predict(corpus_test)\n",
    "cat_f1_test = f1_score(pred_cat_test, target_test)\n",
    "print('F1 модели =', cat_f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:**\n",
    "\n",
    "**В результате проверки модели CatBoostClassifier на тестовых данных - получаем F1-меру 0.75, такой результат нас устраивает.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мы провели исследование с целью создать инструмент для интернет-магазина «Викишоп», который будет искать токсичные комментарии.**\n",
    "\n",
    "**В ходе исследования мы проделали такие шаги:**\n",
    "- после подготовки получили очищенные и лемматизированные данные\n",
    "- создали TF-IDF для обучения моделей\n",
    "- обучили 3 модели, включая 1 модель с градиентным бустингом\n",
    "- провели проверку лучшей модели на тестовых данных\n",
    "\n",
    "**По результатам проверки лучшей модели, а именно CatBoostClassifier, на тестовых данных, мы получили F1-меру = 0.75, рекомендуется использовать именно эту модель для поиска токсичных комментариев.**"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 432,
    "start_time": "2023-03-08T19:45:33.870Z"
   },
   {
    "duration": 4274,
    "start_time": "2023-03-08T19:46:28.986Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-08T19:47:19.276Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-08T19:47:24.081Z"
   },
   {
    "duration": 31,
    "start_time": "2023-03-08T19:47:39.679Z"
   },
   {
    "duration": 38104,
    "start_time": "2023-03-15T19:24:58.799Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-15T19:25:39.812Z"
   },
   {
    "duration": 31,
    "start_time": "2023-03-15T19:25:41.375Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-15T19:25:42.681Z"
   },
   {
    "duration": 8218,
    "start_time": "2023-03-15T19:25:43.546Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-15T19:26:13.456Z"
   },
   {
    "duration": 4690,
    "start_time": "2023-03-15T19:26:23.093Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-15T19:26:29.765Z"
   },
   {
    "duration": 33474,
    "start_time": "2023-03-15T19:27:07.080Z"
   },
   {
    "duration": 2291,
    "start_time": "2023-03-15T19:28:03.368Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-15T19:28:06.892Z"
   },
   {
    "duration": 16229,
    "start_time": "2023-03-15T19:28:08.002Z"
   },
   {
    "duration": 431,
    "start_time": "2023-03-15T19:28:46.504Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-15T19:29:06.794Z"
   },
   {
    "duration": 6918,
    "start_time": "2023-03-15T19:29:18.943Z"
   },
   {
    "duration": 26,
    "start_time": "2023-03-15T19:29:33.707Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-15T19:29:38.206Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
